{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba455bd2",
   "metadata": {},
   "source": [
    "To analyze the SVM approach for classification task we exploited the model provided by **scikit learn** (https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC). The implementation is based on the library **libsvm** (https://www.csie.ntu.edu.tw/~cjlin/libsvm/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18fc5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, GridSearchCV\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b18f81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to return a number with fixed numebr of significant digits\n",
    "\n",
    "from math import log10 , floor\n",
    "\n",
    "def round_it(x, sig):\n",
    "    return round(x, sig-int(floor(log10(abs(x))))-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4d3c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_monks(path):\n",
    "    '''\n",
    "    Function to open monks datasets\n",
    "    Parameters\n",
    "    ---\n",
    "    path : str\n",
    "        It's the path of the file\n",
    "    Returns\n",
    "    ---\n",
    "    monks_df : pandas DataFrame\n",
    "        the df that contains the dataset\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    file = open(path, 'r')\n",
    "    content = file.read().split('\\n') # split to separate different data\n",
    "    monks_df = pd.DataFrame([line.split(' ')[1:] for line in content][:-1]) # creation of the df using separation by ' '\n",
    "    \n",
    "    # The 3 lines below change names to the columns\n",
    "    dict_for_rename = {0:'target', monks_df.shape[1]-1:'id'}\n",
    "    dict_for_rename.update({i:i-1 for i in range(1,monks_df.shape[1]-1)})\n",
    "    monks_df = monks_df.rename(columns=dict_for_rename)\n",
    "    return monks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e19c551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import monk dataset\n",
    "\n",
    "monks1_train = open_monks('MONK/monks-1.train')\n",
    "monks1_test = open_monks('MONK/monks-1.test')\n",
    "\n",
    "monks2_train = open_monks('MONK/monks-2.train')\n",
    "monks2_test = open_monks('MONK/monks-2.test')\n",
    "\n",
    "monks3_train = open_monks('MONK/monks-3.train')\n",
    "monks3_test = open_monks('MONK/monks-3.test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ca399a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hot_encoding(df):\n",
    "    '''\n",
    "    Function to implement one-hot encoding to pandas dataframe\n",
    "    Parameters\n",
    "    ---\n",
    "    df: pandas DataFrame\n",
    "    Return\n",
    "    ---\n",
    "    X_hot: ndarray of input encoded data\n",
    "    y: array of target data\n",
    "    '''\n",
    "    \n",
    "    target_column = df.columns[0] # Columns referred to target \n",
    "    y = df[target_column] # selecting target value for each datapoint\n",
    "    y = y.values # from a pd. Dataframe to a np. array\n",
    "    y = np.array(y, dtype=int) # Convert target values from string to int\n",
    "    \n",
    "    \n",
    "    features_columns = df.columns[1:7] # Columns referred to cat. variables\n",
    "    X = df[features_columns] # selecting features columns for each datapoint   \n",
    "    columns = X.columns # Selecting the columns of X. These columns are just the categorical columns of df \n",
    "    X_hot = pd.get_dummies(X, columns=columns) # applying one-hot encoding to X features (from 6 dims to 17 dims)\n",
    "    X_hot = X_hot.values # from a pd. Dataframe to a np. array\n",
    "    \n",
    "\n",
    "    \n",
    "    return X_hot, y\n",
    "\n",
    "def data_preparation(df):\n",
    "    '''\n",
    "    Function to convert the dataframe to input and target array\n",
    "    Parameters\n",
    "    ---\n",
    "    df: pandas DataFrame\n",
    "    Return\n",
    "    ---\n",
    "    X: ndarray of input data\n",
    "    y: array of target data\n",
    "    '''\n",
    "    \n",
    "    target_column = df.columns[0] # Columns referred to target \n",
    "    y = df[target_column] # selecting target value for each datapoint\n",
    "    y = y.values # from a pd. Dataframe to a np. array\n",
    "    y = np.array(y, dtype=int) # Convert target values from string to int\n",
    "    \n",
    "    \n",
    "    features_columns = df.columns[1:7] # Columns referred to cat. variables\n",
    "    X = df[features_columns].values.astype(int) # selecting features columns for each datapoint   \n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644d911b",
   "metadata": {},
   "source": [
    "### First approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5d639e",
   "metadata": {},
   "source": [
    "- we decided to explore the following kernel: ***rbf***, ***polynomial***, ***sigmoid*** and ***linear***. It's important to specify that for Monk 1, Monk 2 and Monk 3 we used the same procedure, in particular in terms of model selection and assessment; \n",
    "- we used a Repeated stratified k-fold CV for model selection, in particular n=10 repetition and k=5;\n",
    "- for model assessment we exploit an hold-out;\n",
    "- However the scikitlearn library preferes encoded dataset (https://scikit-learn.org/stable/modules/svm.html#tips-on-practical-use on **1.4.5. Tips on Practical Use** paragraph), we try the same procedure used for encoded data for no-encoded data as well. Performance for encoded data are better(in terms of test accuracy) for Monk 1 and Monk 2 but the same as no-encoded data for Monk 3;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9c29bb",
   "metadata": {},
   "source": [
    "### Monk 1: data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b12e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#no one-hot encoding\n",
    "X_design_1, y_design_1 = data_preparation(monks1_train) #for model selection\n",
    "X_test_1, y_test_1 = data_preparation(monks1_test) #for model assessment\n",
    "\n",
    "# with one-hot encoding\n",
    "X_design_enc_1, y_design_enc_1 = hot_encoding(monks1_train)\n",
    "X_test_enc_1, y_test_enc_1 = hot_encoding(monks1_test) \n",
    "\n",
    "#number of samples for Monk1 dataset\n",
    "N_1 = X_design_1.shape[0]  \n",
    "\n",
    "dataset_1 = [[X_design_enc_1, y_design_enc_1, X_test_enc_1, y_test_enc_1], [X_design_1, y_design_1, X_test_1, y_test_1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91986410",
   "metadata": {},
   "source": [
    "### Monk 2: data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98164c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#no one-hot encoding\n",
    "X_design_2, y_design_2 = data_preparation(monks2_train)\n",
    "X_test_2, y_test_2 = data_preparation(monks2_test) \n",
    "\n",
    "# with one-hot encoding\n",
    "X_design_enc_2, y_design_enc_2 = hot_encoding(monks2_train) \n",
    "X_test_enc_2, y_test_enc_2 = hot_encoding(monks2_test) \n",
    "\n",
    "#number of samples for Monk2 dataset\n",
    "N_2 = X_design_2.shape[0]\n",
    "\n",
    "dataset_2 = [[X_design_enc_2, y_design_enc_2, X_test_enc_2, y_test_enc_2], [X_design_2, y_design_2, X_test_2, y_test_2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fd2f65",
   "metadata": {},
   "source": [
    "### Monk 3: data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c427e1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#no one-hot encoding\n",
    "X_design_3, y_design_3 = data_preparation(monks3_train) \n",
    "X_test_3, y_test_3 = data_preparation(monks3_test) \n",
    "\n",
    "# with one-hot encoding\n",
    "X_design_enc_3, y_design_enc_3 = hot_encoding(monks3_train)\n",
    "X_test_enc_3, y_test_enc_3 = hot_encoding(monks3_test) \n",
    "\n",
    "#number of samples for Monk3 dataset\n",
    "N_3 = X_design_3.shape[0] \n",
    "\n",
    "\n",
    "dataset_3 = [[X_design_enc_3, y_design_enc_3, X_test_enc_3, y_test_enc_3], [X_design_3, y_design_3, X_test_3, y_test_3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3c7850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first index: choice of Monk to consider -> Monk 1:0, Monk 2:1, Monk 3:2;\n",
    "# second index: choice of encoded(0) and no-encodend(1) dataset;\n",
    "# thrid index: choice of input,target data for model selection and assessment;\n",
    "dataset = [dataset_1, dataset_2, dataset_3]\n",
    "\n",
    "N=[N_1, N_2, N_3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec19c09f",
   "metadata": {},
   "source": [
    "Choose which Monk you want to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69a2639",
   "metadata": {},
   "outputs": [],
   "source": [
    "monk = 2\n",
    "#after you change this value run the next block(my_grid_search(param_grid, i, monk))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0204e1",
   "metadata": {},
   "source": [
    "**Parameters of SVC:**\n",
    "-  'C': float, default=1.0\n",
    "- 'kernel': {‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’} or callable, default=’rbf’\n",
    "- 'degree': int, default=3\n",
    "- 'gamma': {‘scale’, ‘auto’} or float, default=’scale’\n",
    "- 'coef0': float, default=0.0\n",
    "- 'tol': float, default=1e-3\n",
    "- 'shrinking': bool, default=True            \n",
    "- 'max_iter': int, default=-1 (it means no limit)\n",
    "\n",
    "**shrinking** parameter is a tool that reduces the training time respect to base algorithm (shrinking=False) leading to only small changes,  as theoretically shown in libsvm. For Monk dataset we set shrinking=False because we didn't observe huge changes( it will be very useful for Machine Learning cup). \n",
    "\n",
    "Assumption: others parameters like 'tol' ans 'max_iter' are fixed at their default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c72623",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_rbf= {\n",
    "    \n",
    "    'C': [1e-1, 1, 10],    \n",
    "    \n",
    "    'kernel': ['poly', 'rbf', 'sigmoid'],    \n",
    "    \n",
    "    'gamma': [1e-3 , 1e-2, 1e-1],\n",
    "    'degree': [2, 3, 4, 6],    \n",
    "    'coef0':  [0, 1e-2, 1e-1],  \n",
    "    'shrinking': [False]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6c500d",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_= 0   # 0: encoded data, 1: no-encoded data\n",
    "\n",
    "grid = GridSearchCV(\n",
    "        SVC(),\n",
    "        param_grid=param_grid,\n",
    "        cv=RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=0),\n",
    "        n_jobs=-1,\n",
    "        refit=True\n",
    "    )\n",
    "\n",
    "grid.fit(dataset[monk][i_][0], dataset[monk][i_][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89951e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful dataframes for next analysis\n",
    "\n",
    "a = pd.DataFrame(grid_rbf.cv_results_)\n",
    "\n",
    "b = a_rbf[(a['rank_test_score'] == 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8807c325",
   "metadata": {},
   "source": [
    "### Different approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33254fd",
   "metadata": {},
   "source": [
    "The following code allow us to explore separately each kernel we analyzed previously(i.e. a different GridSerach for each kernel). Moreover we further study the linear kernel. This new approach allow us to avoid useless search, indeed each kernel ignores some of available hyperparameters producing redundant search. Study separately each kernel leads to deeper analysis about the different kernel and in generale the SVC. For example we toke into account the analysis of fraction of support vectors with the goal to choose a model that guarantees good efficiency, in terms of data needed to get a prediction. \n",
    "\n",
    "Assumption: we restrict our analysis to encoded dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1251bb9",
   "metadata": {},
   "source": [
    "### Model selection and assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068fc735",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_grid_search(param_grid, i):\n",
    "    \n",
    "    grid = GridSearchCV(\n",
    "        SVC(),\n",
    "        param_grid=param_grid,\n",
    "        cv=RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=0),\n",
    "        n_jobs=-1,\n",
    "        refit=True,\n",
    "        return_train_score=True\n",
    "    )\n",
    "\n",
    "    return grid.fit(dataset[monk][i][0], dataset[monk][i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746bbbfc",
   "metadata": {},
   "source": [
    "### RBF kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f7bee9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1) choice of hyperpar.'s ranges to explore\n",
    "\n",
    "C_interval_rbf = np.logspace(-4, 4, 9)\n",
    "gamma_interval_rbf = np.logspace(-4, 4, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782d7cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 3) grid search\n",
    "\n",
    "i = 0  # 0: encoded data, 1: no-encoded data\n",
    "\n",
    "\n",
    "param_grid_rbf= {\n",
    "    \n",
    "    'C': C_interval_rbf,    \n",
    "    \n",
    "    'kernel': ['rbf'],  \n",
    "    \n",
    "    'gamma': gamma_interval_rbf,\n",
    "    \n",
    "    'shrinking': [False]\n",
    "}\n",
    "\n",
    "grid_rbf = my_grid_search(param_grid_rbf, i) # grid search already fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea58045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) useful dataframes for next analysis\n",
    "\n",
    "a_rbf = pd.DataFrame(grid_rbf.cv_results_)\n",
    "\n",
    "b_rbf = a_rbf[(a_rbf['rank_test_score'] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad5f462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful code for tasks 5) and 7)\n",
    "\n",
    "def heatmap_rbf(a, score_var):\n",
    "    \n",
    "    k = a[['param_C', 'param_gamma', score_var]]\n",
    "    glue = k.pivot('param_C', 'param_gamma', score_var)\n",
    "    sns.heatmap(glue, cmap=\"crest\", linewidth=.5, annot=True) #annot can be removed if numbers in the cells are bulky\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c901f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) visualization of rbf kernel's grid search(by heatmap): C vs gamma (REALLY INTERESTING!)\n",
    "\n",
    "heatmap_rbf(a_rbf, 'mean_test_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527482a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful code for following tasks: 6) and 9)\n",
    "\n",
    "def plot_rbf(a, b, fix_1, var_1, score_var):\n",
    "    X = np.unique(b[fix_1]) \n",
    "\n",
    "    for j in range(len(X)):\n",
    "        plt.figure(j)\n",
    "        plt.plot(\n",
    "            a[(a[fix_1] == X[j])][var_1],\n",
    "            a[(a[fix_1] == X[j])][score_var]\n",
    "        )\n",
    "        \n",
    "        plt.scatter( \n",
    "            a[(a[fix_1] == X[j])][var_1],\n",
    "            a[(a[fix_1] == X[j])][score_var],\n",
    "            color='b'\n",
    "        )\n",
    "\n",
    "        plt.scatter( \n",
    "            b[(b[fix_1] == X[j])][var_1],\n",
    "            b[(b[fix_1] == X[j])][score_var],\n",
    "            label='best models',\n",
    "            color='r'\n",
    "        )\n",
    "\n",
    "        plt.title('{}={}'.format(fix_1,round_it(X[j], 3)))\n",
    "        plt.xlabel(var_1)\n",
    "        plt.ylabel(score_var)\n",
    "        plt.xscale('log')\n",
    "        plt.grid()\n",
    "\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3127f933",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 6)\n",
    "\n",
    "score_var='mean_test_score'\n",
    "fix_1='param_gamma'\n",
    "var_1='param_C'\n",
    "\n",
    "plot_rbf(a_rbf, b_rbf, fix_1, var_1, score_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7322f7",
   "metadata": {},
   "source": [
    "we can see that the models with the best validation accuracy has the expected relationship between C and gamma in order to\n",
    "avoid underfitting/overfitting:\n",
    "- high gamma requires  low C (to solve possible overfitting)\n",
    "- low gamma requires high C (to solve possible underfitting)\n",
    "- moreover from this grid search visualization I immediately understand what is the hyp. subspace to explore better\n",
    "- For Monk 3: mean of support vectors among the whole grid (a_rbf) and among best model(b_rbf) according to valuation accuracy; in particular we look at sv_a_1 and sv_b_2:\n",
    "    - sv_a_1 =110 +-26\n",
    "    - sv_b_1 = 66 +- 19\n",
    "    \n",
    "    - sv_a_2 = 79+-40\n",
    "    - sv_b_2 = 56+-20 \n",
    "\n",
    "it seems like the best models doesn't have a lowest number of support vectors...\n",
    "Furthermore for this kernel we can observe better this behaviour from heat map for num. support vectors (look at 8) analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030c5195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for support vector analysis\n",
    "\n",
    "def sv_analysis_rbf(df):\n",
    "    hyper_pars = df[['param_C', 'param_gamma']].values\n",
    "    n_support = []\n",
    "    model_list = []\n",
    "    sv_matrix = []\n",
    "\n",
    "    for j in range(len(hyper_pars)):\n",
    "        svc = SVC(\n",
    "            C = hyper_pars[j][0],\n",
    "\n",
    "            kernel='rbf',\n",
    "\n",
    "            gamma = hyper_pars[j][1], \n",
    "\n",
    "            shrinking = False\n",
    "        )\n",
    "        svc.fit(dataset[monk][i][0], dataset[monk][i][1])\n",
    "\n",
    "        sv_matrix.append(np.array([\n",
    "\n",
    "                hyper_pars[j][0], \n",
    "                hyper_pars[j][1], \n",
    "                svc.n_support_.sum()/N[monk]\n",
    "            ]))\n",
    "        model_list.append(svc)\n",
    "\n",
    "    sv_matrix =pd.DataFrame(np.array(sv_matrix))\n",
    "    sv_matrix = sv_matrix.rename(mapper={0:'param_C', 1:'param_gamma', 2:'fsv'}, axis=1)\n",
    "    return sv_matrix, model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8911213b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) support vectors analysis for each cell of the grid search **by heatmap**\n",
    "\n",
    "sv_a = sv_analysis_rbf(a_rbf)[0]\n",
    "\n",
    "heatmap_rbf(sv_a, 'fsv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2621b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) among the best models(with the best validation accuracy) we select the one with the lowest nsv (number of support vector)\n",
    "\n",
    "sv_b, model_list_b = sv_analysis_rbf(b_rbf)\n",
    "rbf_final = model_list_b[np.argmin(sv_b['fsv'])] \n",
    "print('best model chosen:\\n{}'.format(rbf_final))\n",
    "print('number of support vectors:\\n{}'.format(rbf_final.n_support_.sum()))\n",
    "print('fraction of support vectors:\\n{}'.format(round_it(rbf_final.n_support_.sum()/N[monk], 2)))\n",
    "\n",
    "print('validation accuracy:\\n{}'.format(round_it(b_rbf.iloc[np.argmin(sv_b['fsv'])]['mean_test_score'], 3)) )\n",
    "print('test accuracy:\\n{}'.format(round_it(rbf_final.score(dataset[monk][i][2], dataset[monk][i][3]), 3)) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa699f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) \n",
    "\n",
    "sv_a, sv_b = sv_analysis_rbf(a_rbf)[0], sv_analysis_rbf(b_rbf)[0]\n",
    "score_var='n_support_vectors'\n",
    "fix_1='param_gamma'\n",
    "var_1='param_C'\n",
    "\n",
    "plot_rbf(sv_a, sv_b, fix_1, var_1, score_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af41ea81",
   "metadata": {},
   "source": [
    "### Polynomial kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf85235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) choice of hyperpar.'s ranges to explore\n",
    "\n",
    "C_interval_poly = np.logspace(-3, 1, 5)\n",
    "gamma_interval_poly = np.logspace(-1, 3, 5)\n",
    "degree_interval = np.array(range(2, 8, 2))\n",
    "coef0_interval_poly = - np.logspace(-3, 3, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0a687b",
   "metadata": {},
   "source": [
    "For Monk 3 setting odd values for degree cerate a lot of problems. For this reason we exploited only even values( with good performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851ec206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2)\n",
    "\n",
    "param_grid_poly= {\n",
    "    'C': C_interval_poly, \n",
    "    \n",
    "    'kernel': ['poly'],    \n",
    "    \n",
    "    'gamma': gamma_interval_poly, \n",
    "    'degree': degree_interval,    \n",
    "    'coef0':  coef0_interval_poly,  \n",
    "    'shrinking': [True]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b680afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 3) grid search\n",
    "\n",
    "i = 0  # 0:no data encoding, 1: yes data encoding\n",
    "grid_poly = my_grid_search(param_grid_poly, i) # grid search already fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9647dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) useful dataframes for next analysis\n",
    "\n",
    "a_poly = pd.DataFrame(grid_poly.cv_results_)\n",
    "b_poly = a_poly[(a_poly['rank_test_score'] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e80ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful code for tasks 6) and 8.A)\n",
    "\n",
    "def heatmap_poly(a, b, fix_1, fix_2, var_1, var_2, score_var):\n",
    "\n",
    "\n",
    "\n",
    "    X = np.unique(b[[fix_1, fix_2]].values.astype(None), axis=0)\n",
    "\n",
    "    for j in range(len(X)):\n",
    "        plt.figure(j)\n",
    "\n",
    "        matrix = a[(a[fix_1] == X[j][0]) & (a[fix_2] == X[j][1])]\n",
    "        matrix = matrix[[var_1, var_2, score_var]]\n",
    "    \n",
    "        glue = matrix.pivot(var_1, var_2, score_var)\n",
    "        sns.heatmap(glue, cmap=\"crest\", linewidth=.5)\n",
    "        plt.title('{}={}, {}={}'.format(fix_1, X[j][0], fix_2, X[j][1]))\n",
    "\n",
    "    plt.show()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08e20e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) varying 2 hyp. and fixing the others(at the values for the best validation_accuracy)\n",
    "\n",
    "score_var='mean_test_score'\n",
    "\n",
    "fix_1= 'param_coef0'\n",
    "fix_2= 'param_degree'\n",
    "\n",
    "var_1= 'param_C'\n",
    "var_2= 'param_gamma'\n",
    "\n",
    "heatmap_poly(a_poly, b_poly, fix_1, fix_2, var_1, var_2, score_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5708688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for support vectors analysis: \n",
    "# 7) using b_poly, choice of the model with lowest fraction of support vector\n",
    "# 8) using a_poly, number of support vector for each model included in the grid search\n",
    "\n",
    "def sv_analysis_poly(df):\n",
    "    \n",
    "    hyper_pars = df[['param_C', 'param_gamma', 'param_degree', 'param_coef0']].values\n",
    "    n_support = []\n",
    "    model_list = []\n",
    "    sv_matrix = []\n",
    "\n",
    "    for j in range(len(hyper_pars)):\n",
    "        svc = SVC(\n",
    "            C = hyper_pars[j][0],\n",
    "\n",
    "            kernel='poly',\n",
    "\n",
    "            gamma = hyper_pars[j][1],\n",
    "            degree = hyper_pars[j][2],\n",
    "            coef0 = hyper_pars[j][3],\n",
    "\n",
    "            shrinking = False\n",
    "        )\n",
    "        svc.fit(dataset[monk][i][0], dataset[monk][i][1])\n",
    "\n",
    "        sv_matrix.append(\n",
    "            np.array([hyper_pars[j][0], \n",
    "                      hyper_pars[j][1], \n",
    "                      hyper_pars[j][2],\n",
    "                      hyper_pars[j][3],\n",
    "                      svc.n_support_.sum()/N[monk]\n",
    "                     ])\n",
    "        )\n",
    "\n",
    "        model_list.append(svc)\n",
    "\n",
    "    sv_matrix = pd.DataFrame(np.array(sv_matrix))\n",
    "    sv_matrix = sv_matrix.rename(mapper={\n",
    "        0:'param_C', 1:'param_gamma', 2:'param_degree', 3:'param_coef0', 4:'fsv'}, axis=1)\n",
    "    return sv_matrix, model_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b053105",
   "metadata": {},
   "source": [
    "**Some notes:**\n",
    "- mean of support vectors among the whole grid (a_sigmoid) and among best model(b_sigmoid) according to valuation accuracy, in particular we look at sv_a_1 and sv_b_2:\n",
    "    - sv_a_1 = 93+-29\n",
    "    - sv_b_1 = 99+-0.0 (3 models with same num. of sv)\n",
    "    \n",
    "    - sv_a_2 = 101+-27\n",
    "    - sv_b_2 = 99+-0.0 (9 models with same num. of sv) \n",
    "\n",
    "it seems like the best models don't have a lowest number of support vectors..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6648f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) choice of the model with lowest number of support vectors\n",
    "\n",
    "sv_b, model_list_b = sv_analysis_poly(b_poly)\n",
    "\n",
    "poly_final = model_list_b[np.argmin(sv_b['fsv'])] \n",
    "print('best model we choose:\\n{}'.format(poly_final))\n",
    "print('fraction of support vectors:\\n{}'.format(poly_final.n_support_.sum()/N[monk]))\n",
    "print('validation accuracy:\\n{}'.format(round_it(b_poly.iloc[np.argmin(sv_b['fsv'])]['mean_test_score'], 3)) )\n",
    "print('test accuracy:\\n{}'.format(poly_final.score(dataset[monk][i][2], dataset[monk][i][3])) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425d7992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.a) number of support vectors for each model included in the grid search **by heatmap**\n",
    "\n",
    "sv_a, sv_b = sv_analysis_poly(a_poly)[0], sv_analysis_poly(b_poly)[0]\n",
    "\n",
    "score_var='n_support_vectors'\n",
    "\n",
    "fix_1 = 'param_C'\n",
    "fix_2 = 'param_coef0'\n",
    "\n",
    "var_1= 'param_degree'\n",
    "var_2= 'param_gamma'\n",
    "\n",
    "heatmap_poly(sv_a, sv_b, fix_1, fix_2, var_1, var_2, score_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6264a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.b) number of support vectors for each model included in the grid search **by plot of one attribute**\n",
    "\n",
    "sv_a, sv_b = sv_analysis_poly(a_poly)[0], sv_analysis_poly(b_poly)[0]\n",
    "\n",
    "score_var='n_support_vectors'\n",
    "\n",
    "fix_1= 'param_degree'\n",
    "fix_2= 'param_coef0'\n",
    "fix_3= 'param_C'\n",
    "\n",
    "var_1= 'param_gamma'\n",
    "\n",
    "plot_poly(sv_a, sv_b, fix_1, fix_2, fix_3, var_1, score_var, 'log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46880f5c",
   "metadata": {},
   "source": [
    "### Sigmoid kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4524b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's not easy to visualize from b_sigmoid what are the best hyp so:\n",
    "\n",
    "print('C best:\\n{}'.format(np.unique(b_sigmoid['param_C'])))\n",
    "print('gamma best:\\n{}'.format(np.unique(b_sigmoid['param_gamma'])))\n",
    "print('coef0 best:\\n{}'.format(np.unique(b_sigmoid['param_coef0'])))\n",
    "\n",
    "#I use these output in order to set a better grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d5e70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) choice of hyperpar.'s ranges to explore\n",
    "\n",
    "C_interval_sigmoid = np.logspace(-3, 3, 7)\n",
    "gamma_interval_sigmoid = np.logspace(-3, 3, 7)\n",
    "coef0_interval_sigmoid = np.logspace(-3, 3, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bd0a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2)\n",
    "\n",
    "param_grid_sigmoid= {\n",
    "    \n",
    "    'C': C_interval_sigmoid, \n",
    "    \n",
    "    'kernel': ['sigmoid'],  \n",
    "    \n",
    "    'gamma': gamma_interval_sigmoid,\n",
    "    'coef0': coef0_interval_sigmoid,  \n",
    "    \n",
    "    'shrinking': [False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e0f679",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 3) grid search\n",
    "\n",
    "i = 0  # 0:no data encoding, 1: yes data encoding\n",
    "grid_sigmoid = my_grid_search(param_grid_sigmoid, i) # grid search already fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c86e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) useful dataframes for next analysis\n",
    "\n",
    "a_sigmoid = pd.DataFrame(grid_sigmoid.cv_results_)\n",
    "\n",
    "b_sigmoid = a_sigmoid[(a_sigmoid['rank_test_score'] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597f9535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful code for tasks 5) and 8.b)\n",
    "\n",
    "def plot_sigmoid(a, b, fix_1, fix_2, var_1, score_var, scale):\n",
    "\n",
    "    X = np.unique(b[[fix_1 , fix_2]].values.astype(None), axis=0) # it avoids to repeat the same rows\n",
    "\n",
    "    for j in range(len(X)): \n",
    "        plt.figure(j)\n",
    "        plt.plot(\n",
    "            a[(a[fix_1] == X[j][0]) & (a[fix_2] == X[j][1])][var_1],\n",
    "            a[(a[fix_1] == X[j][0]) & (a[fix_2] == X[j][1])][score_var], \n",
    "        )\n",
    "        \n",
    "        plt.scatter( \n",
    "            a[(a[fix_1] == X[j][0]) & (a[fix_2] == X[j][1])][var_1],\n",
    "            a[(a[fix_1] == X[j][0]) & (a[fix_2] == X[j][1])][score_var],\n",
    "            color='b'\n",
    "        )\n",
    "\n",
    "        plt.scatter( \n",
    "            b[(b[fix_1] == X[j][0]) & (b[fix_2] == X[j][1])][var_1],\n",
    "            b[(b[fix_1] == X[j][0]) & (b[fix_2] == X[j][1])][score_var],\n",
    "            label='best models',\n",
    "            color='r'\n",
    "        )\n",
    "        plt.title('{}={}, {}={}'.format(fix_1, X[j][0], fix_2, X[j][1]))\n",
    "        plt.xlabel(var_1)\n",
    "        plt.ylabel(score_var)\n",
    "        plt.xscale(scale)  #pay attention when you set degree!\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "\n",
    "    plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfef7de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) (plot) one hyp. vs validatio_accuracy fixing the others hyp. (at values that maximize the validation_accuracy)\n",
    "\n",
    "score_var='mean_test_score'\n",
    "\n",
    "fix_1= 'param_coef0'\n",
    "fix_2= 'param_C'\n",
    "\n",
    "var_1= 'param_gamma'\n",
    "\n",
    "plot_sigmoid(a_sigmoid, b_sigmoid, fix_1, fix_2, var_1, score_var,'log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eb0ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#useful code for tasks 6) and 8.A)\n",
    "\n",
    "def heatmap_sigmoid(a, b, fix_1, var_1, var_2, score_var):\n",
    "    X = np.unique(b[fix_1].values)  #values for fixed attribute\n",
    "\n",
    "    for j in range(len(X)):\n",
    "\n",
    "        plt.figure(j)\n",
    "        \n",
    "        matrix = a[(a[fix_1] == X[j])]\n",
    "        matrix = matrix[[var_1, var_2, score_var]]\n",
    "        #heatmap in practice\n",
    "        glue = matrix.pivot(var_1, var_2, score_var)\n",
    "        sns.heatmap(glue, cmap=\"crest\", linewidth=.5)\n",
    "        plt.title('for {}_best={}'.format(fix_1, X[j]))\n",
    "\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70188bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) (heatmap) varying 2 hyp. and fixing the third one (at the values for the best validation_accuracy)\n",
    "\n",
    "score_var='mean_test_score'\n",
    "\n",
    "fix_1 = 'param_C'\n",
    "\n",
    "var_1= 'param_coef0'\n",
    "var_2= 'param_gamma'\n",
    "\n",
    "heatmap_sigmoid(a_sigmoid, b_sigmoid, fix_1, var_1, var_2, score_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4ac0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for support vectors analysis: \n",
    "# 7) using b_sigmoid, choice of the model with lowest number of support vector\n",
    "# 8) using a_sigmoid, number of support vector for each model included in the grid search\n",
    "\n",
    "def sv_analysis_sigmoid(df):\n",
    "    \n",
    "    hyper_pars = df[['param_C', 'param_gamma', 'param_coef0']].values\n",
    "    n_support = []\n",
    "    model_list = []\n",
    "    sv_matrix = []\n",
    "\n",
    "    for j in range(len(hyper_pars)):\n",
    "        svc = SVC(\n",
    "            C = hyper_pars[j][0],\n",
    "\n",
    "            kernel='sigmoid',\n",
    "\n",
    "            gamma = hyper_pars[j][1],\n",
    "            coef0 = hyper_pars[j][2],\n",
    "\n",
    "            shrinking = False\n",
    "        )\n",
    "        svc.fit(dataset[monk][i][0], dataset[monk][i][1])\n",
    "\n",
    "        sv_matrix.append(\n",
    "            np.array([hyper_pars[j][0], \n",
    "                      hyper_pars[j][1], \n",
    "                      hyper_pars[j][2],\n",
    "                      svc.n_support_.sum()/N[monk]\n",
    "                     ])\n",
    "        )\n",
    "\n",
    "        model_list.append(svc)\n",
    "\n",
    "    sv_matrix = pd.DataFrame(np.array(sv_matrix))\n",
    "    sv_matrix = sv_matrix.rename(mapper={0:'param_C', 1:'param_gamma', 2:'param_coef0', 3:'fsv'}, axis=1)\n",
    "    return sv_matrix, model_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428c2114",
   "metadata": {},
   "source": [
    "**Some notes:**\n",
    "- we can observe that there isn't a simmetry respect to coef0 sign (look at b_sigmoid)\n",
    "- mean of support vectors among the whole grid (a_sigmoid) and among best model(b_sigmoid) according to valuation accuracy, in particular we look at sv_a_1 and sv_b_2:\n",
    "    - sv_a_1 = 99+-30\n",
    "    - sv_b_1 = 58+-25\n",
    "    - sv_a_2 = 89+-34\n",
    "    - sv_b_2 = 53+-22\n",
    "\n",
    "it seems like the best models has a lower number of support vectors..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3405055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) choice of the model with lowest number of support vector\n",
    "\n",
    "sv_b, model_list_b = sv_analysis_sigmoid(b_sigmoid)\n",
    "\n",
    "sigmoid_final = model_list_b[np.argmin(sv_b['fsv'])] \n",
    "print('best model choosen:\\n{}'.format(sigmoid_final))\n",
    "print('number of support vectors:\\n{}'.format(sigmoid_final.n_support_.sum()))\n",
    "print('fraction of support vectors:\\n{}'.format(round_it(sigmoid_final.n_support_.sum()/N[monk],2)))\n",
    "print('validation accuracy:\\n{}'.format(round_it(b_sigmoid.iloc[np.argmin(sv_b['fsv'])]['std_test_score'], 3)) )\n",
    "print('test accuracy:\\n{}'.format(round_it(sigmoid_final.score(dataset[monk][i][2], dataset[monk][i][3]), 4)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dbcc57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 8.a) number of support vector for each model included in the grid search **by heatmap**\n",
    "\n",
    "sv_a, sv_b = sv_analysis_sigmoid(a_sigmoid)[0], sv_analysis_sigmoid(b_sigmoid)[0]\n",
    "\n",
    "score_var='fsv'\n",
    "\n",
    "fix_1 = 'param_C'\n",
    "\n",
    "var_1= 'param_coef0'\n",
    "var_2= 'param_gamma'\n",
    "\n",
    "heatmap_sigmoid(sv_a, sv_b, fix_1, var_1, var_2, score_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e33f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.B)\n",
    "\n",
    "sv_a, sv_b = sv_analysis_sigmoid(a_sigmoid)[0], sv_analysis_sigmoid(b_sigmoid)[0]\n",
    "\n",
    "score_var='n_support_vectors'\n",
    "\n",
    "fix_1= 'param_coef0'\n",
    "fix_2= 'param_C'\n",
    "\n",
    "var_1= 'param_gamma'\n",
    "\n",
    "plot_sigmoid(sv_a, sv_b, fix_1, fix_2, var_1, score_var,'log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfaeee8",
   "metadata": {},
   "source": [
    "### Linear kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b43248f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) choice of hyperpar.'s ranges to explore\n",
    "\n",
    "C_interval_linear = np.logspace(-6, 6, 13) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5519acd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) \n",
    "param_grid_linear= {\n",
    "    \n",
    "    'C': C_interval_linear, #must be strictly positive     \n",
    "    \n",
    "    'kernel': ['linear'],                   \n",
    "    \n",
    "    'shrinking': [False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfbea9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 3) grid search in practice, first run the previous code!\n",
    "\n",
    "i = 0  # 0:no data encoding, 1: yes data encoding\n",
    "grid_linear = my_grid_search(param_grid_linear, i) # grid search already fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d9fdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) useful dataframes for further analysis\n",
    "\n",
    "a_linear = pd.DataFrame(grid_linear.cv_results_)\n",
    "\n",
    "b_linear = a_linear[(a_linear['rank_test_score'] == 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b82e5c",
   "metadata": {},
   "source": [
    "**Some notes:**\n",
    "- mean of support vectors among the whole grid (a_linear) and among best model(b_linear) according to valuation accuracy, in particular we look at sv_a_1 and sv_b_2:\n",
    "    - sv_a_1 = 82 +- 34\n",
    "    - sv_b_1 = 67 +- 23\n",
    "    - sv_a_2 = 65+-23\n",
    "    - sv_b_2 = 55+-6\n",
    "\n",
    "it seems like the best models have a lowest number of support vectors... BUT look at 8.b) increasing C lowest number of support vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21aced8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful code for task 5) and 8.b)\n",
    "\n",
    "def plot_linear(a, b, score_var):\n",
    "    \n",
    "    plt.plot(a['param_C'], a[score_var])\n",
    "    plt.xlabel('param_C')\n",
    "    plt.scatter(a['param_C'], a[score_var], color='b')\n",
    "    plt.scatter(b['param_C'], b[score_var], color='r', label='best models')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.ylabel(score_var)\n",
    "    plt.xscale('log')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8b1173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) C vs validation_accuracy\n",
    "\n",
    "score_var= 'mean_test_score'\n",
    "\n",
    "plot_linear(a_linear, b_linear, score_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed862e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for support vectors analysis: \n",
    "# 7) using b_linear , choice of the model with lowest number of support vector\n",
    "# 8) using a_linear, number of support vector for each model included in the grid search\n",
    "\n",
    "def sv_analysis_linear(df):\n",
    "    hyper_pars = np.array(df['param_C'])\n",
    "    sv_matrix = []\n",
    "    model_list = []\n",
    "\n",
    "    for j in range(len(hyper_pars)):\n",
    "        svc = SVC(\n",
    "            C = hyper_pars[j],\n",
    "\n",
    "            kernel='linear',\n",
    "\n",
    "            shrinking = False\n",
    "        )\n",
    "        svc.fit(dataset[monk][i][0], dataset[monk][i][1])\n",
    "\n",
    "        sv_matrix.append(np.array([hyper_pars[j],svc.n_support_.sum()/N[monk]]))\n",
    "        model_list.append(svc)\n",
    "\n",
    "    sv_matrix = pd.DataFrame(np.array(sv_matrix)).rename(mapper={\n",
    "    0:'param_C', 1:'fsv'}, axis=1)\n",
    "    return sv_matrix, model_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac9de2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) performance and others information about model with best validation_accuracy and lowest number of support vectors\n",
    "\n",
    "sv_b, model_list_b = sv_analysis_linear(b_linear)\n",
    "linear_final = model_list_b[np.argmin(sv_b['fsv'])] \n",
    "\n",
    "print('best model chosen:\\n{}'.format(linear_final))\n",
    "print('number of support vectors:\\n{}'.format(linear_final.n_support_.sum()))\n",
    "print('fraction of support vectors:\\n{}'.format(linear_final.n_support_.sum()/N_3))\n",
    "print('test accuracy:\\n{}'.format(linear_final.score(dataset[monk][i][2], dataset[monk][i][3])) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1263de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.b)\n",
    "\n",
    "sv_a, sv_b = sv_analysis_linear(a_linear)[0], sv_analysis_linear(b_linear)[0]\n",
    "score_var= 'fsv'\n",
    "\n",
    "plot_linear(sv_a, sv_b, score_var)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
