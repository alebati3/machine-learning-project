{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54876436",
   "metadata": {},
   "source": [
    "# Linear Models - ML CUP (NO LBE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae8ff0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RepeatedKFold, GridSearchCV\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures  \n",
    "from sklearn.base import clone\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    make_scorer,\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaa939c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function calculates the index of the element of \"array\" less distant from \"value\"\n",
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array) # Convert the input to an array.\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return array[idx]\n",
    "\n",
    "\n",
    "\n",
    "# Given a model already trained, this function compute the Loss value \n",
    "# Depending on the regularization type (no regularization , L1, L2) a different kind of loss is computed\n",
    "def loss_f(model_fitted, X, y):\n",
    "    \n",
    "    coeff = model_fitted.coef_   \n",
    "    alpha = model_fitted.get_params([])[\"alpha\"]\n",
    "    \n",
    "    #Not regularized model\n",
    "    if (model_fitted.get_params([])[\"penalty\"] == None): \n",
    "        total_loss = mean_squared_error(y, model_fitted.predict(X))\n",
    "    \n",
    "    #L1 regularization (Lasso)\n",
    "    if (model_fitted.get_params([])[\"penalty\"] == \"l1\"):\n",
    "        total_loss = mean_squared_error(y, model_fitted.predict(X)) + alpha*(np.linalg.norm(coeff, ord = 1))\n",
    "    \n",
    "    #L2 regularization (Ridge)\n",
    "    if (model_fitted.get_params([])[\"penalty\"] == \"l2\"):\n",
    "        total_loss = mean_squared_error(y, model_fitted.predict(X)) +  alpha*(np.dot(coeff, coeff))\n",
    "        \n",
    "    return total_loss\n",
    "\n",
    "\n",
    "# This function is useful to create plots of loss_vs_epochs (over TR) and MSE_vs_epochs (both over TR and VL)\n",
    "def curves(epochs, model, X_train, y_train, X_val, y_val, loss_title, val_title):\n",
    "     \n",
    "    tol = model.get_params([])[\"tol\"]    \n",
    "    epochs_array = np.arange(1, epochs +1, step = 1)    \n",
    "    train_losses = []\n",
    "    train_MSE_s = []    \n",
    "    vl_MSE_s = []\n",
    "    partial_model_list = []\n",
    "    \n",
    "    for i in range(0, epochs):\n",
    "        partial_model_list.append(model.partial_fit(X_train, y_train))\n",
    "    \n",
    "        train_losses.append(loss_f(model, X_train, y_train)) #loss value for i-th epoch\n",
    "        \n",
    "        train_MSE_s.append(mean_squared_error(y_train, model.predict(X_train))) # TR MSE value for i-th epoch\n",
    "        vl_MSE_s.append(mean_squared_error(y_val, model.predict(X_val))) # VL MSE value for i-th epoch\n",
    "    \n",
    "    \n",
    "    #Loss curve\n",
    "    plt.figure()\n",
    "    plt.title(loss_title)\n",
    "    plt.plot(epochs_array, train_losses, label = \"Training\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.savefig(loss_title + \".pdf\")\n",
    "        \n",
    "       \n",
    "    #MSE curves (over the TR and VL)\n",
    "    plt.figure()\n",
    "    plt.title(val_title)\n",
    "    plt.plot(epochs_array, train_MSE_s, color='r', linestyle = \"-\", label = \"Training MSE\")\n",
    "    plt.plot(epochs_array, vl_MSE_s, color='b', linestyle = \"--\", label = \"Validation MSE\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    str_label = \"#epochs at (min(VL MSE) + tol) = \" + str(vl_MSE_s.index(find_nearest(vl_MSE_s, min(vl_MSE_s) + tol)) + 1)\n",
    "    plt.axvline(x = np.array(vl_MSE_s.index(find_nearest(vl_MSE_s, min(vl_MSE_s) + tol)) + 1), color='green', label = str_label)\n",
    "    plt.legend()\n",
    "    plt.grid() \n",
    "    plt.savefig(val_title + \".pdf\")\n",
    "    \n",
    "    \n",
    "    return vl_MSE_s, partial_model_list\n",
    "\n",
    "\n",
    "\n",
    "def plots_f(exp_alpha_min, exp_alpha_max, final_model, n_iteration,\n",
    "            X_inner_tr, y_inner_tr, X_val, y_val,\n",
    "            weights_vs_alpha_title_str, mae_vs_alpha_title_str, \n",
    "            weights_vs_alpha_file_title_name, mae_vs_alpha_file_title_name):\n",
    "    \n",
    "    \n",
    "    coefs = []\n",
    "    alphas = np.logspace(exp_alpha_min, exp_alpha_max, 100)\n",
    "    train_MAEs = []\n",
    "    val_MAEs = []\n",
    "    \n",
    "    #The other hyperparameter are the ones found for the best model during model selection phase\n",
    "    for a in alphas:\n",
    "        model = SGDRegressor(alpha = a, \n",
    "                             eta0 = final_model.get_params([])[\"eta0\"], \n",
    "                            learning_rate =  final_model.get_params([])[\"learning_rate\"],\n",
    "                            loss = final_model.get_params([])[\"loss\"], \n",
    "                            max_iter = final_model.get_params([])[\"max_iter\"], \n",
    "                            n_iter_no_change = final_model.get_params([])[\"n_iter_no_change\"],\n",
    "                             penalty = final_model.get_params([])[\"penalty\"],\n",
    "                             power_t = final_model.get_params([])[\"power_t\"],\n",
    "                            tol =final_model.get_params([])[\"tol\"]\n",
    "                             )\n",
    "        for n in range (n_iteration):\n",
    "            model.partial_fit(X_inner_tr, y_inner_tr)\n",
    "\n",
    "        coefs.append(model.coef_)\n",
    "        train_MAEs.append(mean_absolute_error(y_inner_tr, model.predict(X_inner_tr)))\n",
    "        val_MAEs.append(mean_absolute_error(y_val, model.predict(X_val)))\n",
    "    \n",
    "    \n",
    "    #Plot of linear model coefficients varying the hyperparameter of regularization (alpha).\n",
    "    # The other hyperpar are the same of the best model ones\n",
    "    ax = plt.gca()\n",
    "    ax.plot(alphas, coefs)\n",
    "    ax.set_xscale(\"log\")\n",
    "    plt.xlabel(\"alpha\")\n",
    "    plt.ylabel(\"weights\")\n",
    "    plt.title(weights_vs_alpha_title_str)\n",
    "    plt.grid()\n",
    "    plt.axis(\"tight\")\n",
    "    plt.savefig(weights_vs_alpha_file_title_name + \".pdf\")\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    #Plot of MAE over TR and VL varying alpha, for the best model where.\n",
    "    #The other hyperpar are the same of the best model ones\n",
    "    ax = plt.gca()\n",
    "    ax.plot(alphas, train_MAEs, label = \"TR MAE\")\n",
    "    ax.plot(alphas, val_MAEs, label = \"VL MAE\")\n",
    "    ax.legend()\n",
    "    ax.set_xscale(\"log\")\n",
    "    plt.xlabel(\"alpha\")\n",
    "    plt.ylabel(\"MAE\")\n",
    "    plt.title(mae_vs_alpha_title_str)\n",
    "    plt.grid()\n",
    "    plt.axis(\"tight\")\n",
    "    plt.savefig(mae_vs_alpha_file_title_name + \".pdf\")\n",
    "    plt.show()\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0facb04",
   "metadata": {},
   "source": [
    "# Importing datasets \"ML-CUP22-TR\" and \"ML-CUP22-TS\" (Blind Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d3ca80",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = pd.read_csv('ML-CUP22-TR.csv', skiprows=7, header=None)\n",
    "tr = tr.drop([0], axis=1).rename(columns={i:i-1 for i in tr.columns})\n",
    "blind_ts = pd.read_csv('ML-CUP22-TS.csv', skiprows=7, header=None)\n",
    "blind_ts = blind_ts.drop([0], axis=1).rename(columns={i:i-1 for i in blind_ts.columns})\n",
    " \n",
    "    \n",
    "X_blind = blind_ts.values\n",
    "X = tr.iloc[:,:9].values\n",
    "Y = tr.iloc[:,9:].values\n",
    "\n",
    "# HOLD-OUT TR_&_TS CUP \n",
    "X_tr, X_tt, Y_tr, Y_tt = train_test_split(\n",
    "    X, Y, test_size=0.3, random_state=0, shuffle=True)\n",
    " \n",
    "y1_tr, y2_tr = Y_tr[:,0], Y_tr[:,1]\n",
    "y1_tt, y2_tt = Y_tt[:,0], Y_tt[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb42843d",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "- Linear Basis Expansion: OFF\n",
    "- Data Normalization: ON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19db3eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data normalization so that for each feature mean = 0 and std = 1 (implemented by StandardScaler()\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_tr)\n",
    "X_tr = scaler.transform(X_tr)\n",
    "X_tt = scaler.transform(X_tt)\n",
    "blind_ts = scaler.transform(X_blind)\n",
    "\n",
    "X_tr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e11c91",
   "metadata": {},
   "source": [
    "# Linear Model- No regularization -  Target 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03de840c",
   "metadata": {},
   "source": [
    "## Model Selection (Phase 1)\n",
    "\n",
    "- Goal : Find the best hyperparameters.  \n",
    "- Approach: Grid search through a RepeatedKFold (Repeated: 10 times, #folds: 5)\n",
    "- The RepeatedKFold is applied to the desing test (X_inner_tr, y1_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb598d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "param_grid = {\n",
    "    \"loss\": [\"squared_error\"],\n",
    "    \"penalty\": [None],\n",
    "    \"max_iter\": [3000],\n",
    "    \"tol\": [1e-3, 1e-2, 1e-1],\n",
    "    \"learning_rate\": [\"invscaling\", \"adaptive\"],\n",
    "    \"eta0\": [1e-4, 5e-4],\n",
    "    \"power_t\": [1/4],\n",
    "    \"n_iter_no_change\": [50]          \n",
    "}\n",
    "\n",
    "\n",
    "grid_not_reg1 = GridSearchCV(\n",
    "    SGDRegressor(),\n",
    "    param_grid=param_grid,\n",
    "    scoring = [\"neg_mean_absolute_error\"],\n",
    "    cv=RepeatedKFold(n_splits=5, n_repeats=10, random_state=0),\n",
    "    n_jobs=-1,\n",
    "    refit=\"neg_mean_absolute_error\",\n",
    "    return_train_score = False\n",
    ")\n",
    "\n",
    "grid_not_reg1.fit(X_tr, y1_tr)\n",
    "\n",
    "#Converting grid search results in a pd DataFrame\n",
    "cv_results_ = pd.DataFrame(grid_not_reg1.cv_results_)\n",
    "\n",
    "#Selecting the models with the highest negative mean absolute error (best models)\n",
    "cv_results_[cv_results_[\"rank_test_neg_mean_absolute_error\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e31a0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the grid search results of LM not regularized for target1\n",
    "pd.DataFrame(grid_not_reg1.cv_results_).to_csv(\"grid_not_reg1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c890ff16",
   "metadata": {},
   "source": [
    "## Model Selection (Phase 2)\n",
    "\n",
    "- Goal: find the best number of epochs up to the best model found in the phase 1 has to be train\n",
    "- Approach: Hold-out of the desing test (X_tr, y1_tr): t\n",
    "- The deisgn test is splitted into two subsets: an inner training set 80% (X_inner_tr, y1_inner_tr) and a validation set 20% (X_val, y1_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4a8226",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_inner_tr, X_val, y1_inner_tr, y1_val = train_test_split(X_tr, y1_tr, \n",
    "                                                          test_size=0.2,\n",
    "                                                          random_state=0,\n",
    "                                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779b4485",
   "metadata": {},
   "outputs": [],
   "source": [
    "#max number of epochs \n",
    "n_epochs = 200\n",
    "\n",
    "val_MSE_s_nr1, partial_model_list_nr1 = curves (n_epochs, \n",
    "                                         clone(grid_not_reg1.best_estimator_), \n",
    "                                         X_inner_tr, \n",
    "                                         y1_inner_tr,\n",
    "                                         X_val,\n",
    "                                         y1_val,\n",
    "                                         loss_title = \"Loss - MLCUP - target1 - (LM not regularized)\",\n",
    "                                         val_title =  \"MSE - MLCUP - target1 - (LM not regularized)\" )                                                                                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94e37e7",
   "metadata": {},
   "source": [
    "### Number of epochs up to the best LM not reg (for target 1) will be trained "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150e53db",
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = grid_not_reg1.best_estimator_.get_params([])[\"tol\"]\n",
    "n_iteration_not_reg_1 = val_MSE_s_nr1.index(find_nearest(val_MSE_s_nr1, min(val_MSE_s_nr1)+tol)) + 1\n",
    "print(f\"Epochs di LM not reg finale per target 1: {n_iteration_not_reg_1}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3ba008",
   "metadata": {},
   "source": [
    "### Best model trained for a #epochs equals to n_iteration found in the line of code above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8006ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_fitted_model_not_reg_1 = partial_model_list_nr1[n_iteration_not_reg_1 - 1]\n",
    "final_fitted_model_not_reg_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f19dcc",
   "metadata": {},
   "source": [
    "# Model Assesment - Linear Model - No regularization -  Target 1 \n",
    "\n",
    "- Mean Absolute Error of the model trained over the inner training set (X_inner_tr, y1_inner_tr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463be3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Linear model not reg (target 1): TRAINING_MAE = {mean_absolute_error(y1_inner_tr, final_fitted_model_not_reg_1.predict(X_inner_tr))}\") \n",
    "print(f\"Linear model not reg (target 1): VALIDATION_MAE = {mean_absolute_error(y1_val, final_fitted_model_not_reg_1.predict(X_val))}\") \n",
    "print(f\"Linear model not reg (target 1): TEST_MAE = {mean_absolute_error(y1_tt, final_fitted_model_not_reg_1.predict(X_tt))}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ac6d2c",
   "metadata": {},
   "source": [
    "# Linear Model- No regularization -  Target 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb62e24",
   "metadata": {},
   "source": [
    "## Model Selection (Phase 1)\n",
    "\n",
    "- Goal : Find the best hyperparameters.  \n",
    "- Approach: Grid search through a RepeatedKFold (Repeated: 10 times, #folds: 5)\n",
    "- The RepeatedKFold is applied to the desing test (X_inner_tr, y1_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cf1cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "param_grid = {\n",
    "\n",
    "    \"loss\": [\"squared_error\"],\n",
    "    \"penalty\": [None],\n",
    "    \"max_iter\": [3000],\n",
    "    \"tol\": [1e-4, 1e-3, 1e-2, 1e-1],\n",
    "    \"learning_rate\": [\"invscaling\", \"adaptive\"],\n",
    "    \"eta0\": [1e-4, 5e-4],\n",
    "    \"power_t\": [1/5],\n",
    "    \"n_iter_no_change\": [50]          \n",
    "}\n",
    "\n",
    "\n",
    "grid_not_reg2 = GridSearchCV(\n",
    "    SGDRegressor(),\n",
    "    param_grid=param_grid,\n",
    "    scoring = [\"neg_mean_absolute_error\"],\n",
    "    cv=RepeatedKFold(n_splits=5, n_repeats=10, random_state=0),\n",
    "    n_jobs=-1,\n",
    "    refit=\"neg_mean_absolute_error\",\n",
    "    return_train_score = False\n",
    ")\n",
    "\n",
    "grid_not_reg2.fit(X_tr, y2_tr)\n",
    "\n",
    "#Converting grid search results in a pd DataFrame\n",
    "cv_results_ = pd.DataFrame(grid_not_reg2.cv_results_)\n",
    "\n",
    "#Selecting the models with the highest negative mean absolute error (best models)\n",
    "cv_results_[cv_results_[\"rank_test_neg_mean_absolute_error\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad471081",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the grid search results of LM not regularized for target2\n",
    "pd.DataFrame(grid_not_reg2.cv_results_).to_csv(\"grid_not_reg2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa265035",
   "metadata": {},
   "source": [
    "## Model Selection (Phase 2)\n",
    "\n",
    "- Goal: find the best number of epochs up to the best model found in the phase 1 has to be train\n",
    "- Approach: Hold-out of the desing test (X_tr, y2_tr): t\n",
    "- The deisgn test is splitted into two subsets: an inner training set 80% (X_inner_tr, y2_inner_tr) and a validation set 20% (X_val, y2_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99450a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_inner_tr, X_val, y2_inner_tr, y2_val = train_test_split(X_tr, y2_tr, \n",
    "                                                          test_size=0.2,\n",
    "                                                          random_state=0, \n",
    "                                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae681ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#max number of epochs \n",
    "n_epochs = 200\n",
    "\n",
    "val_MSE_s_nr2, partial_model_list_nr2 = curves (n_epochs, \n",
    "                                         clone(grid_not_reg2.best_estimator_), \n",
    "                                         X_inner_tr, \n",
    "                                         y2_inner_tr,\n",
    "                                         X_val,\n",
    "                                         y2_val,\n",
    "                                         loss_title = \"Loss - MLCUP - target2 - (LM not regularized)\",\n",
    "                                         val_title =  \"MSE - MLCUP - target2 - (LM not regularized)\" )   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b0579b",
   "metadata": {},
   "source": [
    "### Number of epochs up to the best LM not reg (for target 2) will be trained "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d74435d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = grid_not_reg2.best_estimator_.get_params([])[\"tol\"]\n",
    "n_iteration_not_reg_2 = val_MSE_s_nr2.index(find_nearest(val_MSE_s_nr2, min(val_MSE_s_nr2)+tol)) + 1\n",
    "print(f\"Epochs di LM not reg finale per target 1: {n_iteration_not_reg_2}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d6b313",
   "metadata": {},
   "source": [
    "### Best model trained for a #epochs equals to n_iteration found in the line of code above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e07ae53",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_fitted_model_not_reg_2 = partial_model_list_nr2[n_iteration_not_reg_2 - 1]\n",
    "final_fitted_model_not_reg_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa61130",
   "metadata": {},
   "source": [
    "## Model Assesment - Linear Model - No regularization -  Target 2 \n",
    "- Mean Absolute Error of the model trained over the training set inner training set (X_inner_tr, y2_inner_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6594f14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Linear model not reg (target 2): TRAINING_MAE = {mean_absolute_error(y2_inner_tr, final_fitted_model_not_reg_2.predict(X_inner_tr))}\") \n",
    "print(f\"Linear model not reg (target 2): VALIDATION_MAE = {mean_absolute_error(y2_val, final_fitted_model_not_reg_2.predict(X_val))}\") \n",
    "print(f\"Linear model not reg (target 2): TEST_MAE = {mean_absolute_error(y2_tt, final_fitted_model_not_reg_2.predict(X_tt))}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddee457",
   "metadata": {},
   "source": [
    "## Model Assesment - Linear Models - No regularization -  Target 1 and 2 \n",
    "- Assessing the goodness of the models found (LM not reg) for target 1 and 2 by calculating the mean Euclidean error (MEE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63e20c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_MEE_not_reg = (1/X_inner_tr.shape[0])*np.sqrt(np.square(y1_inner_tr - final_fitted_model_not_reg_1.predict(X_inner_tr)) + np.square(y2_inner_tr - final_fitted_model_not_reg_2.predict(X_inner_tr))).sum()\n",
    "VALIDATION_MEE_not_reg = (1/X_val.shape[0])*np.sqrt(np.square(y1_val - final_fitted_model_not_reg_1.predict(X_val)) + np.square(y2_val - final_fitted_model_not_reg_2.predict(X_val))).sum()\n",
    "TEST_MEE_not_reg = (1/X_tt.shape[0])*np.sqrt(np.square(y1_tt - final_fitted_model_not_reg_1.predict(X_tt)) + np.square(y2_tt - final_fitted_model_not_reg_2.predict(X_tt))).sum()\n",
    "\n",
    "print(f\"Linear model not reg: TRAINING_MEE = {TRAINING_MEE_not_reg}\")\n",
    "print(f\"Linear model not reg: VALIDATION_MEE = {VALIDATION_MEE_not_reg}\")\n",
    "print(f\"Linear model not reg: TEST_MEE = {TEST_MEE_not_reg}\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83942670",
   "metadata": {},
   "source": [
    "## Predicting target 1 and 2 of blind test using respectively \"final_fitted_model_not_reg_1\" and \"final_fitted_model_not_reg_2\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac253be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_blind_not_reg = final_fitted_model_not_reg_1.predict(blind_ts)\n",
    "y2_blind_not_reg = final_fitted_model_not_reg_2.predict(blind_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a468531c",
   "metadata": {},
   "source": [
    "# \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf3be83",
   "metadata": {},
   "source": [
    "# Linear Model - L1 regularization (Lasso) - Target 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4372ae43",
   "metadata": {},
   "source": [
    "## Model Selection (Phase 1)\n",
    "\n",
    "- Goal : Find the best hyperparameters.  \n",
    "- Approach: Grid search through a RepeatedKFold (Repeated: 10 times, #folds: 5)\n",
    "- The RepeatedKFold is applied to the desing test (X_inner_tr, y1_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4690f95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "alphas = np.logspace(-3, 0, 100)\n",
    "\n",
    "param_grid = {\n",
    "    \"alpha\": alphas,\n",
    "    \"loss\": [\"squared_error\"],\n",
    "    \"penalty\": [\"l1\"],\n",
    "    \"max_iter\": [3000],\n",
    "    \"tol\": [1e-3],\n",
    "    \"learning_rate\": [\"constant\",\"invscaling\", \"adaptive\"],\n",
    "    \"eta0\": [5e-4],\n",
    "    \"power_t\": [1/4],\n",
    "    \"n_iter_no_change\": [50]          \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "grid_lasso1 = GridSearchCV(\n",
    "    SGDRegressor(),\n",
    "    param_grid=param_grid,\n",
    "    scoring = [\"neg_mean_absolute_error\"],\n",
    "    cv=RepeatedKFold(n_splits=5, n_repeats=10, random_state=0),\n",
    "    n_jobs=-1,\n",
    "    refit=\"neg_mean_absolute_error\",\n",
    "    return_train_score = False\n",
    ")\n",
    "\n",
    "\n",
    "grid_lasso1.fit(X_tr, y1_tr)\n",
    "\n",
    "#Converting grid search results in a pd DataFrame\n",
    "cv_results_ = pd.DataFrame(grid_lasso1.cv_results_)\n",
    "\n",
    "#Selecting the models with the highest negative mean absolute error (best models)\n",
    "cv_results_[cv_results_[\"rank_test_neg_mean_absolute_error\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f113737",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the grid search results of LM LASSO for target1\n",
    "pd.DataFrame(grid_lasso1.cv_results_).to_csv(\"grid_lasso1.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fce06bb",
   "metadata": {},
   "source": [
    "## Model Selection (Phase 2)\n",
    "\n",
    "- Goal: find the best number of epochs up to the best model found in the phase 1 has to be train\n",
    "- Approach: Hold-out of the desing test (X_tr, y1_tr): t\n",
    "- The deisgn test is splitted into two subsets: an inner training set 80% (X_inner_tr, y1_inner_tr) and a validation set 20% (X_val, y1_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad88baaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#max number of epochs \n",
    "n_epochs = 200\n",
    "\n",
    "val_MSE_s_lasso1, partial_model_list_lasso1  =   curves (n_epochs, \n",
    "                                                 clone(grid_lasso1.best_estimator_), \n",
    "                                                 X_inner_tr, \n",
    "                                                 y1_inner_tr,\n",
    "                                                 X_val,\n",
    "                                                 y1_val,\n",
    "                                                 loss_title = \"Loss - MLCUP - target1 - (LASSO)\",\n",
    "                                                 val_title =  \"MSE - MLCUP - target1 - (LASSO)\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc4aeff",
   "metadata": {},
   "source": [
    "### Number of epochs up to the best LM LASSO (for target 1) will be trained \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197c51a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = grid_lasso1.best_estimator_.get_params([])[\"tol\"]\n",
    "n_iteration_lasso_1 = val_MSE_s_lasso1.index(find_nearest(val_MSE_s_lasso1, min(val_MSE_s_lasso1)+tol)) + 1\n",
    "print(f\"Epochs di LM LASSO finale per target 1: {n_iteration_lasso_1}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e388bb4",
   "metadata": {},
   "source": [
    "### Best LM LASSO (for target 1) trained for a #epochs equals to n_iteration_lasso_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1440fd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_fitted_model_lasso_1 = partial_model_list_lasso1[n_iteration_lasso_1 - 1]\n",
    "final_fitted_model_lasso_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0358e2",
   "metadata": {},
   "source": [
    "### Plots to study how the regularization hyperparameter affects the final values of weights and the mean absolute errore (MAE) over the inner training set (X_inner_tr, y1_inner_tr) and validation set (X_val, y1_val)\n",
    "- The other hyperparameters used for building these plots are setted equal to the ones found for the best model \"final_fitted_model_lasso_1\". \n",
    "- For every value of alpha a clone of \"final_fitted_model_lasso_1\" (clone: same model but still untrained) has beeen trained up to #epochs = n_iteration_lasso_1.\n",
    "- After the training:\n",
    "- 1) Final weights has been extracted; --> (1st plot)\n",
    "- 2) Has been computed the mean absolute error over inner training and validation --> (2nd plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94079820",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_f(-3, 2, final_fitted_model_lasso_1, n_iteration_lasso_1,\n",
    "            X_inner_tr, y1_inner_tr, X_val, y1_val,\n",
    "            \"LASSO coefficients as a function of alpha (Target 1)\", \"MLCUP - Target1 - (LASSO)\",\n",
    "            \"coeff_vs_alpha_lasso\", \"mae_vs_alpha_lasso1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f28167",
   "metadata": {},
   "source": [
    "# Model Assesment -Linear Model - L1 regularization (Lasso) - Target 1\n",
    "- Mean Absolute Error of the model trained over the training set inner training set (X_inner_tr, y1_inner_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f41d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Linear model LASSO (target 1): TRAINING_MAE = {mean_absolute_error(y1_inner_tr, final_fitted_model_lasso_1.predict(X_inner_tr))}\") \n",
    "print(f\"Linear model LASSO (target 1): VALIDATION_MAE = {mean_absolute_error(y1_val, final_fitted_model_lasso_1.predict(X_val))}\") \n",
    "print(f\"Linear model LASSO (target 1): TEST_MAE = {mean_absolute_error(y1_tt, final_fitted_model_lasso_1.predict(X_tt))}\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc7fb2f",
   "metadata": {},
   "source": [
    "# Linear Model - L1 regularization (Lasso) - Target 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b628f33",
   "metadata": {},
   "source": [
    "## Model Selection (Phase 1)\n",
    "\n",
    "- Goal : Find the best hyperparameters.  \n",
    "- Approach: Grid search through a RepeatedKFold (Repeated: 10 times, #folds: 5)\n",
    "- The RepeatedKFold is applied to the desing test (X_inner_tr, y2_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab2dd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "alphas = np.logspace(-3, 0, 100)\n",
    "\n",
    "param_grid = {\n",
    "    \"alpha\": alphas,\n",
    "    \"loss\": [\"squared_error\"],\n",
    "    \"penalty\": [\"l1\"],\n",
    "    \"max_iter\": [3000],\n",
    "    \"tol\": [1e-3],\n",
    "    \"learning_rate\": [\"constant\",\"invscaling\", \"adaptive\"],\n",
    "    \"eta0\": [5e-4],\n",
    "    \"power_t\": [1/4],\n",
    "    \"n_iter_no_change\": [100]          \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "grid_lasso2 = GridSearchCV(\n",
    "    SGDRegressor(),\n",
    "    param_grid=param_grid,\n",
    "    scoring = [\"neg_mean_absolute_error\"],\n",
    "    cv=RepeatedKFold(n_splits=5, n_repeats=10, random_state=0),\n",
    "    n_jobs=-1,\n",
    "    refit=\"neg_mean_absolute_error\",\n",
    "    return_train_score = False\n",
    ")\n",
    "\n",
    "\n",
    "grid_lasso2.fit(X_tr, y2_tr)\n",
    "\n",
    "#Converting grid search results in a pd DataFrame\n",
    "cv_results_ = pd.DataFrame(grid_lasso2.cv_results_)\n",
    "\n",
    "#Selecting the models with the highest negative mean absolute error (best models)\n",
    "cv_results_[cv_results_[\"rank_test_neg_mean_absolute_error\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50936941",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the grid search results of LM LASSO for target2\n",
    "pd.DataFrame(grid_lasso2.cv_results_).to_csv(\"grid_lasso2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e377097",
   "metadata": {},
   "source": [
    "## Model Selection (Phase 2)\n",
    "\n",
    "- Goal: find the best number of epochs up to the best model found in the phase 1 has to be train\n",
    "- Approach: Hold-out of the desing test (X_tr, y2_tr): t\n",
    "- The deisgn test is splitted into two subsets: an inner training set 80% (X_inner_tr, y2_inner_tr) and a validation set 20% (X_val, y2_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1812fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#max number of epochs \n",
    "n_epochs = 200 \n",
    "\n",
    "val_MSE_s_lasso2, partial_model_list_lasso2 = curves (n_epochs ,                  \n",
    "                                         clone(grid_lasso2.best_estimator_), \n",
    "                                         X_inner_tr, \n",
    "                                         y2_inner_tr,\n",
    "                                         X_val,\n",
    "                                         y2_val,\n",
    "                                         loss_title = \"Loss - MLCUP - target2 - (LASSO)\",\n",
    "                                         val_title =  \"MSE - MLCUP - target2 - (LASSO)\" )  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a12f89c",
   "metadata": {},
   "source": [
    "### Number of epochs up to the best LM LASSO (for target 2) will be trained \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54f2506",
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = grid_lasso2.best_estimator_.get_params([])[\"tol\"]\n",
    "n_iteration_lasso_2 = val_MSE_s_lasso2.index(find_nearest(val_MSE_s_lasso2, min(val_MSE_s_lasso2)+tol)) + 1\n",
    "print(f\"Epochs di LM LASSO finale per target 2: {n_iteration_lasso_2}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f315301",
   "metadata": {},
   "source": [
    "### Training the best LM LASSO (for target 2) for a #epochs equals to n_iteration_lasso_2\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b729f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_fitted_model_lasso_2 = partial_model_list_lasso2[n_iteration_lasso_2 - 1]\n",
    "final_fitted_model_lasso_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e2d3b8",
   "metadata": {},
   "source": [
    "### Plots to study how the regularization hyperparameter affects the final values of weights and the mean absolute errore (MAE) over the inner training set (X_inner_tr, y2_inner_tr) and validation set (X_val, y2_val)\n",
    "- The other hyperparameters used for building these plots are setted equal to the ones found for the best model \"final_fitted_model_lasso_2\". \n",
    "- For every value of alpha a clone of \"final_fitted_model_lasso_2\" (clone: same model but still untrained) has beeen trained up to #epochs = n_iteration_lasso_2.\n",
    "- After the training:\n",
    "- 1) Final weights has been extracted; --> (1st plot)\n",
    "- 2) Has been computed the mean absolute error over inner training and validation --> (2nd plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de469067",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_f(-5, 2, final_fitted_model_lasso_2, n_iteration_lasso_2,\n",
    "            X_inner_tr, y2_inner_tr, X_val, y2_val,\n",
    "            \"Lasso coefficients as a function of alpha (Target 2)\", \"MLCUP - Target2 - (LASSO)\",\n",
    "            \"coeff_vs_alpha_lasso2\", \"mae_vs_alpha_lasso2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f294f6b",
   "metadata": {},
   "source": [
    "# Model Assesment -Linear Model - L1 regularization (Lasso) - Target 2\n",
    "- Mean Absolute Error of the model trained over the training set inner training set (X_inner_tr, y1_inner_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2488c876",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Linear model LASSO (target 2): TRAINING_MAE = {mean_absolute_error(y2_inner_tr, final_fitted_model_lasso_2.predict(X_inner_tr))}\") \n",
    "print(f\"Linear model LASSO (target 2): VALIDATION_MAE = {mean_absolute_error(y2_val, final_fitted_model_lasso_2.predict(X_val))}\") \n",
    "print(f\"Linear model LASSO (target 2): TEST_MAE = {mean_absolute_error(y2_tt, final_fitted_model_lasso_2.predict(X_tt))}\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ba11bd",
   "metadata": {},
   "source": [
    "# Model Assesment - Linear Models - LASSO -  Target 1 and 2 \n",
    "- Assessing the goodness of the models found (LM LASSO) for target 1 and 2 by calculating the mean Euclidean error (MEE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb032a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_MEE_lasso = (1/X_inner_tr.shape[0])*np.sqrt(np.square(y1_inner_tr - final_fitted_model_lasso_1.predict(X_inner_tr)) + np.square(y2_inner_tr - final_fitted_model_lasso_2.predict(X_inner_tr))).sum()\n",
    "VALIDATION_MEE_lasso= (1/X_val.shape[0])*np.sqrt(np.square(y1_val - final_fitted_model_lasso_1.predict(X_val)) + np.square(y2_val - final_fitted_model_lasso_2.predict(X_val))).sum()\n",
    "TEST_MEE_lasso = (1/X_tt.shape[0])*np.sqrt(np.square(y1_tt - final_fitted_model_lasso_1.predict(X_tt)) + np.square(y2_tt - final_fitted_model_lasso_2.predict(X_tt))).sum()\n",
    "\n",
    "print(f\"Linear model LASSO: TRAINING_MEE = {TRAINING_MEE_lasso}\")\n",
    "print(f\"Linear model LASSO: VALIDATION_MEE = {VALIDATION_MEE_lasso}\")\n",
    "print(f\"Linear model LASSO: TEST_MEE = {TEST_MEE_lasso}\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be1bfe5",
   "metadata": {},
   "source": [
    "##  Predicting target 1 and 2 of blind test using respectively \"final_fitted_model_lasso_1\" and \"final_fitted_model_lasso_2\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4efc846",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_blind_lasso= final_fitted_model_lasso_1.predict(blind_ts)\n",
    "y2_blind_lasso = final_fitted_model_lasso_2.predict(blind_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bc3e3c",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53549a47",
   "metadata": {},
   "source": [
    "# Linear Model - L2 regularization (Ridge) - Target 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a64657",
   "metadata": {},
   "source": [
    "## Model Selection (Phase 1)\n",
    "\n",
    "- Goal : Find the best hyperparameters.  \n",
    "- Approach: Grid search through a RepeatedKFold (Repeated: 10 times, #folds: 5)\n",
    "- The RepeatedKFold is applied to the desing test (X_inner_tr, y1_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8bb8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "alphas = np.logspace(-3, 0, 100)\n",
    "\n",
    "param_grid = {\n",
    "    \"alpha\": alphas,\n",
    "    \"loss\": [\"squared_error\"],\n",
    "    \"penalty\": [\"l2\"],\n",
    "    \"max_iter\": [3000],\n",
    "    \"tol\": [1e-3, 1e-2, 1e-1],\n",
    "    \"learning_rate\": [\"constant\",\"invscaling\", \"adaptive\"],\n",
    "    \"eta0\": [1e-4, 5e-4],\n",
    "    \"power_t\": [1/4],\n",
    "    \"n_iter_no_change\": [100]          \n",
    "}\n",
    "\n",
    "\n",
    "grid_ridge1 = GridSearchCV(\n",
    "    SGDRegressor(),\n",
    "    param_grid=param_grid,\n",
    "    scoring = [\"neg_mean_absolute_error\"],\n",
    "    cv=RepeatedKFold(n_splits=5, n_repeats=10, random_state=0),\n",
    "    n_jobs=-1,\n",
    "    refit=\"neg_mean_absolute_error\",\n",
    "    return_train_score = False\n",
    ")\n",
    "\n",
    "\n",
    "grid_ridge1.fit(X_tr, y1_tr)\n",
    "\n",
    "#Converting grid search results in a pd DataFrame\n",
    "cv_results_ = pd.DataFrame(grid_ridge1.cv_results_)\n",
    "\n",
    "#Selecting the models with the highest negative mean absolute error (best models)\n",
    "cv_results_[cv_results_[\"rank_test_neg_mean_absolute_error\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de88d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the grid search results of LM RIDGE for target1\n",
    "pd.DataFrame(grid_ridge1.cv_results_).to_csv(\"grid_ridge1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e08c72",
   "metadata": {},
   "source": [
    "## Model Selection (Phase 2)\n",
    "\n",
    "- Goal: find the best number of epochs up to the best model found in the phase 1 has to be train\n",
    "- Approach: Hold-out of the desing test (X_tr, y1_tr): t\n",
    "- The deisgn test is splitted into two subsets: an inner training set 80% (X_inner_tr, y1_inner_tr) and a validation set 20% (X_val, y1_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475958cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#max number of epochs \n",
    "n_epochs = 200\n",
    "\n",
    "val_MSE_s_ridge1, partial_model_list_ridge1 = curves (n_epochs ,                  \n",
    "                                         clone(grid_ridge1.best_estimator_), \n",
    "                                         X_inner_tr, \n",
    "                                         y1_inner_tr,\n",
    "                                         X_val,\n",
    "                                         y1_val,\n",
    "                                         loss_title = \"Loss - MLCUP - target1 - (RIDGE)\",\n",
    "                                         val_title =  \"MSE - MLCUP - target1 - (RIDGE)\" )  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cb5ffd",
   "metadata": {},
   "source": [
    "### Number of epochs up to the best LM RIDGE (for target 1) will be trained \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092994dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = grid_ridge1.best_estimator_.get_params([])[\"tol\"]\n",
    "n_iteration_ridge_1 = val_MSE_s_ridge1.index(find_nearest(val_MSE_s_ridge1, min(val_MSE_s_ridge1)+tol)) + 1\n",
    "print(f\"Epochs di LM RIDGE finale per target 1: {n_iteration_ridge_1}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deb2bc1",
   "metadata": {},
   "source": [
    "### Best LM RIDGE (for target 1) trained for a #epochs equals to n_iteration found in the line of code above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89791d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_fitted_model_ridge_1 = partial_model_list_ridge1[n_iteration_ridge_1 - 1]\n",
    "final_fitted_model_ridge_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc796eac",
   "metadata": {},
   "source": [
    "### Plots to study how the regularization hyperparameter affects the final values of weights and the mean absolute errore (MAE) over the inner training set (X_inner_tr, y1_inner_tr) and validation set (X_val, y1_val)\n",
    "- The other hyperparameters used for building these plots are setted equal to the ones found for the best model \"final_fitted_model_ridge_1\". \n",
    "- For every value of alpha a clone of \"final_fitted_model_ridge_1\" (clone: same model but still untrained) has beeen trained up to #epochs = n_iteration_ridge_1.\n",
    "- After the training:\n",
    "- 1) Final weights has been extracted; --> (1st plot)\n",
    "- 2) Has been computed the mean absolute error over inner training and validation --> (2nd plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86ff89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_f(-3, 3, final_fitted_model_ridge_1, n_iteration_ridge_1,\n",
    "            X_inner_tr, y1_inner_tr, X_val, y1_val,\n",
    "            \"RIDGE coefficients as a function of alpha (Target 1)\", \"MLCUP - Target1 - (RIDGE)\",\n",
    "            \"coeff_vs_alpha_ridge1\", \"mae_vs_alpha_ridge1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edba7876",
   "metadata": {},
   "source": [
    "# Model Assesment -Linear Model - L2 regularization (Ridge) - Target 1\n",
    "- Mean Absolute Error of the model trained over the training set inner training set (X_inner_tr, y1_inner_tr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c0cc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Linear model RIDGE (target 1): TRAINING_MAE = {mean_absolute_error(y1_inner_tr, final_fitted_model_ridge_1.predict(X_inner_tr))}\") \n",
    "print(f\"Linear model RIDGE (target 1): VALIDATION_MAE = {mean_absolute_error(y1_val, final_fitted_model_ridge_1.predict(X_val))}\") \n",
    "print(f\"Linear model RIDGE (target 1): TEST_MAE = {mean_absolute_error(y1_tt, final_fitted_model_ridge_1.predict(X_tt))}\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac60dd2",
   "metadata": {},
   "source": [
    "# Linear Model - L2 regularization (Ridge) - Target 2 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81311eea",
   "metadata": {},
   "source": [
    "## Model Selection (Phase 1)\n",
    "\n",
    "- Goal : Find the best hyperparameters.  \n",
    "- Approach: Grid search through a RepeatedKFold (Repeated: 10 times, #folds: 5)\n",
    "- The RepeatedKFold is applied to the desing test (X_inner_tr, y2_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843edb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "alphas = np.logspace(-3, 0, 100)\n",
    "\n",
    "param_grid = {\n",
    "    \"alpha\": alphas,\n",
    "    \"loss\": [\"squared_error\"],\n",
    "    \"penalty\": [\"l2\"],\n",
    "    \"max_iter\": [3000],\n",
    "    \"tol\": [1e-3, 1e-2, 1e-1],\n",
    "    \"learning_rate\": [\"constant\",\"invscaling\", \"adaptive\"],\n",
    "    \"eta0\": [1e-4, 5e-4],\n",
    "    \"power_t\": [1/4],\n",
    "    \"n_iter_no_change\": [100]          \n",
    "}\n",
    "\n",
    "\n",
    "grid_ridge2 = GridSearchCV(\n",
    "    SGDRegressor(),\n",
    "    param_grid=param_grid,\n",
    "    scoring = [\"neg_mean_absolute_error\"],\n",
    "    cv=RepeatedKFold(n_splits=5, n_repeats=10, random_state=0),\n",
    "    n_jobs=-1,\n",
    "    refit=\"neg_mean_absolute_error\",\n",
    "    return_train_score = False\n",
    ")\n",
    "\n",
    "\n",
    "grid_ridge2.fit(X_tr, y2_tr)\n",
    "\n",
    "#Converting grid search results in a pd DataFrame\n",
    "cv_results_ = pd.DataFrame(grid_ridge2.cv_results_)\n",
    "\n",
    "#Selecting the models with the highest negative mean absolute error (best models)\n",
    "cv_results_[cv_results_[\"rank_test_neg_mean_absolute_error\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e537be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the grid search results of LM RIDGE for target2\n",
    "pd.DataFrame(grid_ridge2.cv_results_).to_csv(\"grid_ridge2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935f9772",
   "metadata": {},
   "source": [
    "## Model Selection (Phase 2)\n",
    "\n",
    "- Goal: find the best number of epochs up to the best model found in the phase 1 has to be train\n",
    "- Approach: Hold-out of the desing test (X_tr, y2_tr): t\n",
    "- The deisgn test is splitted into two subsets: an inner training set 80% (X_inner_tr, y2_inner_tr) and a validation set 20% (X_val, y2_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24324ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#max number of epochs \n",
    "n_epochs = 200\n",
    "\n",
    "val_MSE_s_ridge2, partial_model_list_ridge2 = curves (n_epochs ,                  \n",
    "                                         clone(grid_ridge2.best_estimator_), \n",
    "                                         X_inner_tr, \n",
    "                                         y2_inner_tr,\n",
    "                                         X_val,\n",
    "                                         y2_val,\n",
    "                                         loss_title = \"Loss - MLCUP - target2 - (RIDGE)\",\n",
    "                                         val_title =  \"MSE - MLCUP - target2 - (RIDGE)\" )  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b02f364",
   "metadata": {},
   "source": [
    "### Number of epochs up to the best LM RIDGE (for target 2) will be trained \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3260bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = grid_ridge2.best_estimator_.get_params([])[\"tol\"]\n",
    "n_iteration_ridge_2 = val_MSE_s_ridge2.index(find_nearest(val_MSE_s_ridge2, min(val_MSE_s_ridge2)+tol)) + 1\n",
    "print(f\"Epochs di LM RIDGE finale per target 1: {n_iteration_ridge_2}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd966b4",
   "metadata": {},
   "source": [
    "### Best LM RIDGE (for target 2) trained for a #epochs equals to n_iteration found in the line of code above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e237884",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_fitted_model_ridge_2 = partial_model_list_ridge2[n_iteration_ridge_2 - 1]\n",
    "final_fitted_model_ridge_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793d828b",
   "metadata": {},
   "source": [
    "### Plots to study how the regularization hyperparameter affects the final values of weights and the mean absolute errore (MAE) over the inner training set (X_inner_tr, y2_inner_tr) and validation set (X_val, y2_val)\n",
    "- The other hyperparameters used for building these plots are setted equal to the ones found for the best model \"final_fitted_model_ridge_2\". \n",
    "- For every value of alpha a clone of \"final_fitted_model_ridge_2\" (clone: same model but still untrained) has beeen trained up to #epochs = n_iteration_ridge_2.\n",
    "- After the training:\n",
    "- 1) Final weights has been extracted; --> (1st plot)\n",
    "- 2) Has been computed the mean absolute error over inner training and validation --> (2nd plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212d7df8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plots_f(-5, 3, final_fitted_model_ridge_2, n_iteration_ridge_2,\n",
    "            X_inner_tr, y2_inner_tr, X_val, y2_val,\n",
    "            \"Ridge coefficients as a function of alpha (Target 2)\", \"MLCUP - Target2 - (RIDGE)\",\n",
    "            \"coeff_vs_alpha_ridge2\", \"mae_vs_alpha_ridge2\")\n",
    "                                         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329a9f6f",
   "metadata": {},
   "source": [
    "# Model Assesment -Linear Model - L2 regularization (Ridge) - Target 2\n",
    "- Mean Absolute Error of the model trained over the training set inner training set (X_inner_tr, y2_inner_tr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7ae72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Linear model RIDGE (target 2): TRAINING_MAE = {mean_absolute_error(y2_inner_tr, final_fitted_model_ridge_2.predict(X_inner_tr))}\") \n",
    "print(f\"Linear model RIDGE (target 2): VALIDATION_MAE = {mean_absolute_error(y2_val, final_fitted_model_ridge_2.predict(X_val))}\") \n",
    "print(f\"Linear model RIDGE (target 2): TEST_MAE = {mean_absolute_error(y2_tt, final_fitted_model_ridge_2.predict(X_tt))}\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869a6036",
   "metadata": {},
   "source": [
    "## Assessing the goodness of the models (LM RIDGE) found by calculating the Mean Euclidean Error (MEE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a45e31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_MEE_ridge = (1/X_inner_tr.shape[0])*np.sqrt(np.square(y1_inner_tr - final_fitted_model_ridge_1.predict(X_inner_tr)) + np.square(y2_inner_tr - final_fitted_model_ridge_2.predict(X_inner_tr))).sum()\n",
    "VALIDATION_MEE_ridge= (1/X_val.shape[0])*np.sqrt(np.square(y1_val - final_fitted_model_ridge_1.predict(X_val)) + np.square(y2_val - final_fitted_model_ridge_2.predict(X_val))).sum()\n",
    "TEST_MEE_ridge = (1/X_tt.shape[0])*np.sqrt(np.square(y1_tt - final_fitted_model_ridge_1.predict(X_tt)) + np.square(y2_tt - final_fitted_model_ridge_2.predict(X_tt))).sum()\n",
    "\n",
    "print(f\"Linear model RIDGE: TRAINING_MEE = {TRAINING_MEE_ridge}\")\n",
    "print(f\"Linear model RIDGE: VALIDATION_MEE = {VALIDATION_MEE_ridge}\")\n",
    "print(f\"Linear model RIDGE: TEST_MEE = {TEST_MEE_ridge}\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7872a464",
   "metadata": {},
   "source": [
    "##  Predicting target 1 and 2 of blind test using respectively \"final_fitted_model_ridge_1\" and \"final_fitted_model_ridge_2\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb673805",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_blind_ridge = final_fitted_model_ridge_1.predict(blind_ts)\n",
    "y2_blind_ridge = final_fitted_model_ridge_2.predict(blind_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1495a18a",
   "metadata": {},
   "source": [
    "## Saving precition over blind test for each model in a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d083b4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blind_predictions_LMs = (pd.DataFrame([y1_blind_not_reg, \n",
    "               y2_blind_not_reg, \n",
    "               y1_blind_lasso, \n",
    "               y2_blind_lasso, \n",
    "               y1_blind_ridge, \n",
    "               y2_blind_ridge]).T).rename(columns={0: \"y1_blind_not_reg\",\n",
    "                                                   1: \"y2_blind_not_reg\",\n",
    "                                                   2: \"y1_blind_lasso\",\n",
    "                                                   3: \"y2_blind_lasso\",\n",
    "                                                   4: \"y1_blind_ridge\", \n",
    "                                                   5: \"y2_blind_ridge\"})\n",
    "\n",
    "df_blind_predictions_LMs.to_csv('blind_predictions_LMs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c7665d",
   "metadata": {},
   "source": [
    "# Best model combination (Lasso for target 1 and ridge for target 2)\n",
    "- Computing the MEE of the model combination over TR, VL and TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbaa703",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_train_mee = (1/X_inner_tr.shape[0])*np.sqrt(np.square(y1_inner_tr - final_fitted_model_lasso_1.predict(X_inner_tr)) + np.square(y2_inner_tr - final_fitted_model_ridge_2.predict(X_inner_tr))).sum()\n",
    "combined_validation_mee = (1/X_val.shape[0])*np.sqrt(np.square(y1_val - final_fitted_model_lasso_1.predict(X_val)) + np.square(y2_val - final_fitted_model_ridge_2.predict(X_val))).sum()\n",
    "combined_test_mee = (1/X_tt.shape[0])*np.sqrt(np.square(y1_tt - final_fitted_model_ridge_1.predict(X_tt)) + np.square(y2_tt - final_fitted_model_ridge_2.predict(X_tt))).sum()\n",
    "\n",
    "\n",
    "print(f\"Linear model combined (lasso1 + ridge2): TRAINING_MEE = {combined_train_mee}\") \n",
    "print(f\"Linear model combined (lasso1 + ridge2): VALIDATION_MEE = {combined_validation_mee}\") \n",
    "print(f\"Linear model combined (lasso1 + ridge2): TEST_MEE = {combined_test_mee}\") \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
